{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import xml.etree.ElementTree as etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the raw xml of WIKI EN-ZH 405K pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_white_spaces(txt):\n",
    "    \"\"\"Collapse multiple white spaces into one white space\n",
    "    (from lazyNLP)\n",
    "    \"\"\"\n",
    "    clean_txt = ''\n",
    "    prev = None\n",
    "    for c in txt:\n",
    "        if c == ' ' and prev == ' ':\n",
    "            continue\n",
    "        else:\n",
    "            clean_txt += c\n",
    "        prev = c\n",
    "    return clean_txt\n",
    "\n",
    "def connect_lines(txt, line_sep='\\n'):\n",
    "    \"\"\" This happens when you crawl text from a webpage and\n",
    "    they have random breaking lines mid-sentence.\n",
    "    This function is to connect those lines.\n",
    "    Two consecutive lines are seperated by line_sep.\n",
    "    (from lazyNLP)\n",
    "    \"\"\"\n",
    "    lines = txt.split('\\n')\n",
    "\n",
    "    result, curr = '', ''\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if curr:\n",
    "                result += (curr + '\\n')\n",
    "            result += line_sep\n",
    "            curr = ''\n",
    "        else:\n",
    "            curr += (line + ' ')\n",
    "\n",
    "    return result + curr\n",
    "\n",
    "def clean_html(txt):\n",
    "    \"\"\" Clean HTML tags of webpages downloaded\n",
    "    Use this function for Gutenberg book format.\n",
    "    (from lazyNLP)\n",
    "    \"\"\"\n",
    "    style_tag_re = re.compile('<style.*?>[^<>]*?</style>')\n",
    "    txt = re.sub(style_tag_re, '', txt)\n",
    "    script_tag_re = re.compile('<script.*?>[^<>]*?</script>')\n",
    "    txt = re.sub(script_tag_re, '', txt)\n",
    "    doc_tag_re = re.compile('<!DOCTYPE[^<>]*?>')\n",
    "    txt = re.sub(doc_tag_re, '', txt)\n",
    "    html_tag_re = re.compile('<.*?>')\n",
    "    txt = connect_lines(txt)\n",
    "    txt = collapse_white_spaces(txt)\n",
    "    \n",
    "    return html.unescape(re.sub(html_tag_re, '', txt).strip())\n",
    "\n",
    "def extract_EN_ZH_documents_from_pair(pair):\n",
    "    en_regex = r\"<article lang=\\\"en\\\"(\\n|.)*<\\/article>\\n\"\n",
    "    matches = re.finditer(en_regex, pair, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        en = clean_html(match.group())\n",
    "    \n",
    "    zh_regex = r\"<article lang=\\\"zh\\\"(\\n|.)*<\\/article>\"\n",
    "    matches = re.finditer(zh_regex, pair, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        zh = clean_html(match.group())\n",
    "    return en, zh\n",
    "\n",
    "def extract_EN_JA_documents_from_pair(pair):\n",
    "    en_regex = r\"<article lang=\\\"en\\\"(\\n|.)*<\\/article>\\n\"\n",
    "    matches = re.finditer(en_regex, pair, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        en = clean_html(match.group())\n",
    "    \n",
    "    ja_regex = r\"<article lang=\\\"ja\\\"(\\n|.)*<\\/article>\"\n",
    "    matches = re.finditer(ja_regex, pair, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        ja = clean_html(match.group())\n",
    "    return en, ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swallow my memory\n",
    "\"\"\"\n",
    "file = \"/home/ponshane/Downloads/wikicomp-2014_enzh.xml\"\n",
    "handler = open(file)\n",
    "html_string = \"\"\n",
    "for line in handler:\n",
    "    html_string += line\n",
    "handler.close()\n",
    "\"\"\"\n",
    "\n",
    "# bs4 does not support iterparse... also takes all memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" wikicomp-2014_enzh.xml\n",
    "fname = \"/home/ponshane/Downloads/wikicomp-2014_enzh.xml\"\n",
    "EN_Articles = []\n",
    "ZH_Articles = []\n",
    "with open(fname) as f:\n",
    "    temp_pair = \"\"\n",
    "    collect_flag = False\n",
    "    for line in f:\n",
    "        if \"</articlePair>\" in line:\n",
    "            collect_flag = False\n",
    "            en, zh = extract_EN_ZH_documents_from_pair(temp_pair)\n",
    "            EN_Articles.append(en)\n",
    "            ZH_Articles.append(zh)\n",
    "            assert len(EN_Articles) == len(ZH_Articles)\n",
    "            if len(ZH_Articles) % 10000 == 0:\n",
    "                print(\"Have processed: \", len(ZH_Articles))\n",
    "            temp_pair = \"\"\n",
    "            #break\n",
    "        elif \"<articlePair id=\" in line:\n",
    "            collect_flag = True\n",
    "        elif collect_flag == True:\n",
    "            temp_pair += line\n",
    "\"\"\"\n",
    "\n",
    "fname = \"/home/ponshane/Downloads/wikicomp-2014_enja.xml\"\n",
    "EN_Articles = []\n",
    "JA_Articles = []\n",
    "with open(fname) as f:\n",
    "    temp_pair = \"\"\n",
    "    collect_flag = False\n",
    "    for line in f:\n",
    "        if \"</articlePair>\" in line:\n",
    "            collect_flag = False\n",
    "            en, ja = extract_EN_JA_documents_from_pair(temp_pair)\n",
    "            EN_Articles.append(en)\n",
    "            JA_Articles.append(ja)\n",
    "            assert len(EN_Articles) == len(JA_Articles)\n",
    "            if len(JA_Articles) % 10000 == 0:\n",
    "                print(\"Have processed: \", len(JA_Articles))\n",
    "            temp_pair = \"\"\n",
    "            #break\n",
    "        elif \"<articlePair id=\" in line:\n",
    "            collect_flag = True\n",
    "        elif collect_flag == True:\n",
    "            temp_pair += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# en_zh\n",
    "# with open('wiki_en_zh_405k.pickle', 'wb') as f:\n",
    "#     # Pickle the 'data' dictionary using the highest protocol available.\n",
    "#     pickle.dump((EN_Articles, ZH_Articles), f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# en_ja\n",
    "with open('wiki_en_ja_393k.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump((EN_Articles, JA_Articles), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store in Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "### init and read config\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./config.ini')\n",
    "\n",
    "MongoDB = config[\"ADM\"][\"Database\"]\n",
    "MongoUser = config[\"ADM\"][\"User\"]\n",
    "MongoPW = config[\"ADM\"][\"PW\"]\n",
    "\n",
    "###連接MONGO\n",
    "uri = \"mongodb://\" + MongoUser + \":\" + MongoPW + \"@140.117.69.70:30241/\" +\\\n",
    "MongoDB + \"?authMechanism=SCRAM-SHA-1\"\n",
    "\n",
    "client = MongoClient(uri)\n",
    "db = client.ComparableWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have stored:  0\n",
      "Have stored:  10000\n",
      "Have stored:  20000\n",
      "Have stored:  30000\n",
      "Have stored:  40000\n",
      "Have stored:  50000\n",
      "Have stored:  60000\n",
      "Have stored:  70000\n",
      "Have stored:  80000\n",
      "Have stored:  90000\n",
      "Have stored:  100000\n",
      "Have stored:  110000\n",
      "Have stored:  120000\n",
      "Have stored:  130000\n",
      "Have stored:  140000\n",
      "Have stored:  150000\n",
      "Have stored:  160000\n",
      "Have stored:  170000\n",
      "Have stored:  180000\n",
      "Have stored:  190000\n",
      "Have stored:  200000\n",
      "Have stored:  210000\n",
      "Have stored:  220000\n",
      "Have stored:  230000\n",
      "Have stored:  240000\n",
      "Have stored:  250000\n",
      "Have stored:  260000\n",
      "Have stored:  270000\n",
      "Have stored:  280000\n",
      "Have stored:  290000\n",
      "Have stored:  300000\n",
      "Have stored:  310000\n",
      "Have stored:  320000\n",
      "Have stored:  330000\n",
      "Have stored:  340000\n",
      "Have stored:  350000\n",
      "Have stored:  360000\n",
      "Have stored:  370000\n",
      "Have stored:  380000\n",
      "Have stored:  390000\n"
     ]
    }
   ],
   "source": [
    "# en_zh\n",
    "# assert len(EN_Articles) == len(ZH_Articles)\n",
    "# for idx, _ in enumerate(EN_Articles):\n",
    "#     db.ENZH.insert_one({\"PairNumber\":idx, \"EN-Content\":EN_Articles[idx], \"ZH-Content\":ZH_Articles[idx]})\n",
    "#     if idx % 10000 == 0:\n",
    "#         print(\"Have stored: \", idx)\n",
    "\n",
    "assert len(EN_Articles) == len(JA_Articles)\n",
    "for idx, _ in enumerate(EN_Articles):\n",
    "    db.ENJA.insert_one({\"PairNumber\":idx, \"EN-Content\":EN_Articles[idx], \"JA-Content\":JA_Articles[idx]})\n",
    "    if idx % 10000 == 0:\n",
    "        print(\"Have stored: \", idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-lingual",
   "language": "python",
   "name": "cross-lingual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
